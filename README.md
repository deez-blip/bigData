# Big Data Project â€” Pipeline ELT (Pandas & Spark)

Ce dÃ©pÃ´t contient deux projets rÃ©alisÃ©s dans le cadre du cours de Big Data :

**1) AmÃ©lioration de la pipeline** *(sans Machine Learning)*  
**2) Big Data avec Spark** *(Spark cluster via Docker + transformations Spark)*

---

## PrÃ©requis

Avant de lancer le projet, assure-toi dâ€™avoir :

- Python + environnement virtuel (`.venv`)
- Docker + Docker Compose
- Prefect installÃ© (pour lâ€™orchestration)
- Streamlit installÃ© (dashboard)

---

## Lancer Prefect

AprÃ¨s Ãªtre entrÃ© dans ton environnement Python :

```bash
prefect server start
```

â¡ï¸ Lâ€™interface Prefect sera accessible sur :  
`http://localhost:4200`

---

## Pipeline Pandas (Bronze / Silver / Gold)

ğŸ“Œ **Objectif : tester la pipeline complÃ¨te avec Pandas**

### Ã‰tapes :

**Terminal 1 : lancer le watcher Pandas**
```bash
python scripts/watch_sources.py
```

**Terminal 2 : gÃ©nÃ©rer les donnÃ©es**
```bash
python scripts/generate_data.py
```

ğŸ“Œ La gÃ©nÃ©ration actuelle produit environ **1 million de lignes** dans chaque CSV.  
â¡ï¸ Le watcher dÃ©tecte lâ€™ajout/modification des fichiers `.csv` dans `data/sources` et dÃ©clenche automatiquement :

Bronze â†’ Silver â†’ Gold

---

## Pipeline Spark (Big Data)

ğŸ“Œ **Objectif : remplacer Pandas par PySpark sur Silver et Gold**

### Ã‰tapes :

**Terminal 1 : lancer le watcher Spark**
```bash
python scripts/watch_sources_spark.py
```

**Terminal 2 : gÃ©nÃ©rer les donnÃ©es**
```bash
python scripts/generate_data.py
```

â¡ï¸ Le watcher dÃ©tecte les changements et dÃ©clenche automatiquement :

Bronze â†’ Silver Spark â†’ Gold Spark

---

## Benchmark Pandas vs Spark (Comparatif performance)

ğŸ“Œ **Objectif : comparer le temps de traitement entre Pandas et Spark**

Assure-toi dâ€™avoir dÃ©jÃ  des fichiers dans `data/sources/` (clients / achats / promo)  
Puis lance :

```bash
python scripts/benchmark_pandas_vs_spark.py
```

â¡ï¸ Le script exÃ©cute les 2 pipelines et affiche le temps de traitement :

- Bronze
- Silver
- Gold
- Total

Un fichier CSV de rÃ©sultats est aussi gÃ©nÃ©rÃ© :

ğŸ“„ `data/benchmark_results.csv`

---

## Dashboard Streamlit

Le dashboard Streamlit affiche automatiquement les rÃ©sultats du **dernier run Gold**.

Il se base sur le fichier pointeur :

ğŸ“Œ `gold/latest_run.json`

â¡ï¸ Ce fichier contient le `run_id` et le `gold_prefix` Ã  lire, ce qui permet de garder un historique des runs sans Ã©craser les prÃ©cÃ©dents.

---

## RÃ©sumÃ© des scripts importants

| Script | RÃ´le |
|-------|------|
| `scripts/generate_data.py` | GÃ©nÃ¨re des CSV (â‰ˆ 1M lignes) avec quelques erreurs volontaires |
| `scripts/watch_sources.py` | Watcher auto-run pipeline Pandas |
| `scripts/watch_sources_spark.py` | Watcher auto-run pipeline Spark |
| `scripts/benchmark_pandas_vs_spark.py` | Comparatif temps Pandas vs Spark |
| `dashboard/streamlit_app.py` | Dashboard Streamlit connectÃ© Ã  MinIO |

---

## Notes

- Le projet utilise un systÃ¨me de **run_id** pour historiser les runs.
- Les layers Bronze / Silver / Gold sont stockÃ©es dans MinIO.
- Le systÃ¨me est conÃ§u pour Ãªtre **automatique** : ajout/modification de CSV â†’ pipeline relancÃ©e.

---
